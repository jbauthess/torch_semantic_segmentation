{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo : using UNET to segment chest X-ray images (JSRT segmentation02 dataset)\n",
    "\n",
    "In this notebook, **we're going to build a simple UNET model** to segment chest X-ray images using 247 images from the \"Standard Digital Image Database\" of the Japan Society of Japan Radiological Technology ([JSRT](http://imgcom.jsrt.or.jp/minijsrtdb/)).\n",
    "\n",
    "It is a 4-labels segmentation problem where pixels belong to different organs. To each image is associated a mask (ground-truth) containing 255 for pixels in the lung area, 85 for pixels in the heart area, 170 for pixels in the lung field, and 0 for pixels in vitro.\n",
    "\n",
    "Note : There is no medical basis for the definition and determination of the lung area of the label data\n",
    "because it has not been medically supervised.\n",
    "\n",
    "We will:\n",
    "* `instanciate the datasets` - during this step image and mask data will be downloaded if missing\n",
    "* `train the model` - fit the model on images \n",
    "* `test and evaluate` - test model prediction and compute segmentation scores\n",
    "\n",
    "A custom data-augmenter (AugmentedSemanticSegmentationDataset) is used to generate additional samples from the existing ones\n",
    "during the training process. This augmenter is applying RANDOM:\n",
    "- colorimetric transformation ONLY to image \n",
    "- spatial transformation to image AND associated mask\n",
    "\n",
    "This simple use case can be modified to check quickly the implementation of a custom model architecture.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device : cuda\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path \n",
    "\n",
    "import torch\n",
    "\n",
    "from src.model.unet import UNetModel\n",
    "from src.dataset.jsrt_dataset import JsrtSegmentation02Dataset\n",
    "from src.dataset.data_augmentation.color_augmentation import ColorimetricAugmentationParams\n",
    "from src.dataset.data_augmentation.spatial_augmentation import SpatialAugmentationParams\n",
    "from src.dataset.data_augmentation.augmented_semantic_segmentation import (\n",
    "    AugmentedSemanticSegmentationDataset,\n",
    ")\n",
    "from src.benchmark.train import train, HyperParameters, EarlyStoppingParams, TrainParameters\n",
    "\n",
    "\n",
    "# algorithm hyperparameters\n",
    "# -------------------------\n",
    "# we use a large value here in order to train the model\n",
    "# until early stopping is triggered\n",
    "NB_EPOCHS = 20 \n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "# model hyperparameters\n",
    "# ---------------------\n",
    "# for the simple task at hand the base number (20) used to compute the number of feature maps\n",
    "# in each model layer can be low. In the original paper this value is 64\n",
    "BASE_FM_NUMBER = 20\n",
    "IMG_NB_CHANNELS = 1 # grayscale images\n",
    "NB_LABELS = 4       # pixels can belong to one of 4 labels\n",
    "INIT_WEIGHTS_PATH = None\n",
    "\n",
    "MODEL_PATH = \"./square_model_dict.pth\"\n",
    "DATASET_FOLDER_PATH = Path(\"./jsrt\") \n",
    "\n",
    "# deactivate PIL debug ifo polutting output...\n",
    "pil_logger = logging.getLogger(\"PIL\")\n",
    "pil_logger.setLevel(logging.INFO)\n",
    "\n",
    "logging.basicConfig(filename=None, filemode=\"w\", level=logging.DEBUG)\n",
    "\n",
    "# select device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Selected device : {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Early stopping DEACTIVATED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Init model weights using the default initialization strategy\n",
      "INFO:root:initialize loss and optimizer for a model predicting 4 labels\n",
      "INFO:root:selected loss function: CrossEntropyLoss()\n",
      "INFO:root:Epoch [1/20],Loss: 1.3781, Val Loss: 0.0000\n",
      "INFO:root:Epoch 0, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [2/20],Loss: 0.6482, Val Loss: 0.0000\n",
      "INFO:root:Epoch 1, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [3/20],Loss: 0.3954, Val Loss: 0.0000\n",
      "INFO:root:Epoch 2, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [4/20],Loss: 0.2851, Val Loss: 0.0000\n",
      "INFO:root:Epoch 3, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [5/20],Loss: 0.2083, Val Loss: 0.0000\n",
      "INFO:root:Epoch 4, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [6/20],Loss: 0.1801, Val Loss: 0.0000\n",
      "INFO:root:Epoch 5, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [7/20],Loss: 0.1531, Val Loss: 0.0000\n",
      "INFO:root:Epoch 6, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [8/20],Loss: 0.1440, Val Loss: 0.0000\n",
      "INFO:root:Epoch 7, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [9/20],Loss: 0.1295, Val Loss: 0.0000\n",
      "INFO:root:Epoch 8, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [10/20],Loss: 0.1199, Val Loss: 0.0000\n",
      "INFO:root:Epoch 9, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [11/20],Loss: 0.1170, Val Loss: 0.0000\n",
      "INFO:root:Epoch 10, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [12/20],Loss: 0.1088, Val Loss: 0.0000\n",
      "INFO:root:Epoch 11, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [13/20],Loss: 0.1051, Val Loss: 0.0000\n",
      "INFO:root:Epoch 12, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [14/20],Loss: 0.1028, Val Loss: 0.0000\n",
      "INFO:root:Epoch 13, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [15/20],Loss: 0.1026, Val Loss: 0.0000\n",
      "INFO:root:Epoch 14, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [16/20],Loss: 0.0942, Val Loss: 0.0000\n",
      "INFO:root:Epoch 15, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [17/20],Loss: 0.0886, Val Loss: 0.0000\n",
      "INFO:root:Epoch 16, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [18/20],Loss: 0.0858, Val Loss: 0.0000\n",
      "INFO:root:Epoch 17, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [19/20],Loss: 0.0870, Val Loss: 0.0000\n",
      "INFO:root:Epoch 18, Learning Rate: 0.000500\n",
      "INFO:root:Epoch [20/20],Loss: 0.0820, Val Loss: 0.0000\n",
      "INFO:root:Epoch 19, Learning Rate: 0.000500\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model on a synthetic dataset composed of squares with different colors (1-label problem) \"\"\"\n",
    "\n",
    "# build model architecture\n",
    "model = UNetModel(IMG_NB_CHANNELS, NB_LABELS, BASE_FM_NUMBER)\n",
    "\n",
    "# create train and validation datasets \n",
    "# Note that the first run may take a longer time (around 2 minutes) because \n",
    "# dataset needs to be downloaded\n",
    "current_dir = Path(os.getcwd())\n",
    "train_dataset = JsrtSegmentation02Dataset(DATASET_FOLDER_PATH, download=True, image_set=\"train\")\n",
    "validation_dataset = None\n",
    "\n",
    "# Add data augmentation\n",
    "spatial_params = SpatialAugmentationParams()\n",
    "colorimetric_params = ColorimetricAugmentationParams()\n",
    "train_dataset = AugmentedSemanticSegmentationDataset(\n",
    "    train_dataset, spatial_params, colorimetric_params\n",
    ")\n",
    "\n",
    "if validation_dataset:\n",
    "    validation_dataset = AugmentedSemanticSegmentationDataset(\n",
    "        validation_dataset, spatial_params, colorimetric_params\n",
    "    )\n",
    "\n",
    "# set training hyperparameters\n",
    "hyperparameters = HyperParameters(\n",
    "    NB_EPOCHS, BATCH_SIZE, LEARNING_RATE, None# EarlyStoppingParams(3, 0.001)\n",
    ")\n",
    "\n",
    "# run the training process\n",
    "train(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    TrainParameters(hyperparameters, INIT_WEIGHTS_PATH, device),\n",
    "    MODEL_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:matplotlib data path: d:\\user\\JB\\code\\TorchSemanticSegmentation\\.venv\\Lib\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=C:\\Users\\authe\\.matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is win32\n",
      "DEBUG:matplotlib:CACHEDIR=C:\\Users\\authe\\.matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from C:\\Users\\authe\\.matplotlib\\fontlist-v390.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\" test the model on the jsrt dataset composed of Chest X-ray images (4-labels problem)  \"\"\"\n",
    "from src.benchmark.test import test, EvalMetrics, TestReport\n",
    "\n",
    "# build model architecture and load weights generated during the training process\n",
    "model = UNetModel(1, NB_LABELS, BASE_FM_NUMBER).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))\n",
    "\n",
    "# get the \"test\" dataset\n",
    "current_dir = Path(os.getcwd())\n",
    "test_dataset = train_dataset = JsrtSegmentation02Dataset(\n",
    "    DATASET_FOLDER_PATH, download=True, image_set=\"test\"\n",
    ")\n",
    "\n",
    "# Configure test report : micro-averaged AND per-label metrics\n",
    "test_report = TestReport(\n",
    "    current_dir.joinpath(\"report.json\"),\n",
    "    [\n",
    "        EvalMetrics.ACCURACY,\n",
    "        EvalMetrics.RECALL_PER_LABEL,\n",
    "        EvalMetrics.PRECISION_PER_LABEL,\n",
    "        EvalMetrics.F1_SCORE_PER_LABEL,\n",
    "        EvalMetrics.IOU_PER_LABEL,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# run evaluation (with segmentation image display deactivated : verbise = 0)\n",
    "test(\n",
    "    device,\n",
    "    model,\n",
    "    test_dataset,\n",
    "    test_report,\n",
    "    verbose=0,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchsemanticsegmentation-py3.12 (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
